# coding=utf-8
"""æ–°é—»æŠ¥å‘Šç”Ÿæˆæ¨¡å—"""

import os
import json
import tempfile
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple
from datetime import datetime

from src.models.news import News, WordGroupStatistic
from src.utils.file import clean_title, html_escape
from src.utils.time import format_time_filename, format_date_folder


class NewsReporter:
    """æ–°é—»æŠ¥å‘Šç”Ÿæˆå™¨

    è´Ÿè´£ï¼š
    - å‡†å¤‡æŠ¥å‘Šæ•°æ®
    - æ ¼å¼åŒ–æ ‡é¢˜
    - ç”Ÿæˆå„ç§æ ¼å¼çš„æŠ¥å‘Šï¼ˆHTMLã€æ–‡æœ¬ç­‰ï¼‰
    """

    def __init__(self, rank_threshold: int = 10):
        """åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨

        Args:
            rank_threshold: æ’åé˜ˆå€¼ï¼ˆç”¨äºé«˜äº®æ˜¾ç¤ºï¼‰
        """
        self.rank_threshold = rank_threshold

    def prepare_report_data(
        self,
        stats: List[WordGroupStatistic],
        failed_ids: Optional[List[str]] = None,
        new_news_list: Optional[List[News]] = None,
        mode: str = "daily"
    ) -> Dict:
        """å‡†å¤‡æŠ¥å‘Šæ•°æ®

        Args:
            stats: è¯ç»„ç»Ÿè®¡åˆ—è¡¨
            failed_ids: å¤±è´¥çš„å¹³å°IDåˆ—è¡¨
            new_news_list: æ–°å¢æ–°é—»åˆ—è¡¨
            mode: æ¨¡å¼ (daily/current/incremental)

        Returns:
            Dict: æŠ¥å‘Šæ•°æ®å­—å…¸
        """
        # åœ¨å¢é‡æ¨¡å¼ä¸‹éšè—æ–°å¢æ–°é—»åŒºåŸŸ
        hide_new_section = mode == "incremental"

        # å¤„ç†æ–°å¢æ–°é—»
        processed_new_titles = []
        if not hide_new_section and new_news_list:
            # æŒ‰å¹³å°åˆ†ç»„
            new_by_platform = {}
            for news in new_news_list:
                platform_name = news.platform_name
                if platform_name not in new_by_platform:
                    new_by_platform[platform_name] = []
                new_by_platform[platform_name].append(news)

            for platform_name, news_list in new_by_platform.items():
                source_titles = []
                for news in news_list:
                    processed_title = {
                        "title": news.title,
                        "source_name": news.platform_name,
                        "time_display": "",
                        "count": 1,
                        "ranks": [news.rank],
                        "rank_threshold": self.rank_threshold,
                        "url": news.url,
                        "mobile_url": news.mobile_url or "",
                        "is_new": True,
                    }
                    source_titles.append(processed_title)

                if source_titles:
                    processed_new_titles.append({
                        "source_id": news.platform,
                        "source_name": platform_name,
                        "titles": source_titles,
                    })

        # å¤„ç†ç»Ÿè®¡æ•°æ®
        processed_stats = []
        for stat in stats:
            if stat.count <= 0:
                continue

            processed_titles = []
            for news in stat.news_list:
                # ä»extraä¸­è·å–ä¿¡æ¯
                extra = news.extra
                processed_title = {
                    "title": news.title,
                    "source_name": news.platform_name,
                    "time_display": extra.get("time_display", ""),
                    "count": extra.get("count", 1),
                    "ranks": extra.get("all_ranks", [news.rank]),
                    "rank_threshold": self.rank_threshold,
                    "url": news.url,
                    "mobile_url": extra.get("mobileUrl", ""),
                    "is_new": extra.get("is_new", False),
                }
                processed_titles.append(processed_title)

            processed_stats.append({
                "word": stat.word,
                "count": stat.count,
                "percentage": stat.percentage,
                "titles": processed_titles,
            })

        return {
            "stats": processed_stats,
            "new_titles": processed_new_titles,
            "failed_ids": failed_ids or [],
            "total_new_count": sum(
                len(source["titles"]) for source in processed_new_titles
            ),
        }

    def format_rank_display(
        self,
        ranks: List[int],
        rank_threshold: int,
        format_type: str
    ) -> str:
        """ç»Ÿä¸€çš„æ’åæ ¼å¼åŒ–æ–¹æ³•

        Args:
            ranks: æ’ååˆ—è¡¨
            rank_threshold: æ’åé˜ˆå€¼
            format_type: æ ¼å¼ç±»å‹ (html/feishu/dingtalk/wework/telegram/ntfy)

        Returns:
            str: æ ¼å¼åŒ–åçš„æ’åæ˜¾ç¤º
        """
        if not ranks:
            return ""

        unique_ranks = sorted(set(ranks))
        min_rank = unique_ranks[0]
        max_rank = unique_ranks[-1]

        # æ ¹æ®å¹³å°é€‰æ‹©é«˜äº®æ ¼å¼
        if format_type == "html":
            highlight_start = "<font color='red'><strong>"
            highlight_end = "</strong></font>"
        elif format_type == "feishu":
            highlight_start = "<font color='red'>**"
            highlight_end = "**</font>"
        elif format_type in ["dingtalk", "wework"]:
            highlight_start = "**"
            highlight_end = "**"
        elif format_type == "telegram":
            highlight_start = "<b>"
            highlight_end = "</b>"
        else:
            highlight_start = "**"
            highlight_end = "**"

        # åˆ¤æ–­æ˜¯å¦é«˜äº®
        is_highlight = min_rank <= rank_threshold

        if min_rank == max_rank:
            rank_text = f"[{min_rank}]"
        else:
            rank_text = f"[{min_rank} - {max_rank}]"

        if is_highlight:
            return f"{highlight_start}{rank_text}{highlight_end}"
        else:
            return rank_text

    def format_title_for_platform(
        self,
        platform: str,
        title_data: Dict,
        show_source: bool = True
    ) -> str:
        """ç»Ÿä¸€çš„æ ‡é¢˜æ ¼å¼åŒ–æ–¹æ³•

        Args:
            platform: å¹³å°ç±»å‹ (feishu/dingtalk/wework/telegram/ntfy/html)
            title_data: æ ‡é¢˜æ•°æ®å­—å…¸
            show_source: æ˜¯å¦æ˜¾ç¤ºæ¥æºå¹³å°

        Returns:
            str: æ ¼å¼åŒ–åçš„æ ‡é¢˜
        """
        rank_display = self.format_rank_display(
            title_data["ranks"],
            title_data["rank_threshold"],
            platform
        )

        link_url = title_data["mobile_url"] or title_data["url"]
        cleaned_title = clean_title(title_data["title"])
        title_prefix = "ğŸ†• " if title_data.get("is_new") else ""

        if platform == "feishu":
            return self._format_feishu(
                cleaned_title, link_url, title_prefix, title_data, rank_display, show_source
            )
        elif platform == "dingtalk":
            return self._format_dingtalk(
                cleaned_title, link_url, title_prefix, title_data, rank_display, show_source
            )
        elif platform == "wework":
            return self._format_wework(
                cleaned_title, link_url, title_prefix, title_data, rank_display, show_source
            )
        elif platform == "telegram":
            return self._format_telegram(
                cleaned_title, link_url, title_prefix, title_data, rank_display, show_source
            )
        elif platform == "ntfy":
            return self._format_ntfy(
                cleaned_title, link_url, title_prefix, title_data, rank_display, show_source
            )
        elif platform == "html":
            return self._format_html(
                cleaned_title, link_url, title_data, rank_display
            )
        else:
            return cleaned_title

    def _format_feishu(
        self,
        title: str,
        link_url: str,
        prefix: str,
        data: Dict,
        rank_display: str,
        show_source: bool
    ) -> str:
        """é£ä¹¦æ ¼å¼"""
        if link_url:
            formatted_title = f"[{title}]({link_url})"
        else:
            formatted_title = title

        if show_source:
            result = f"<font color='grey'>[{data['source_name']}]</font> {prefix}{formatted_title}"
        else:
            result = f"{prefix}{formatted_title}"

        if rank_display:
            result += f" {rank_display}"
        if data["time_display"]:
            result += f" <font color='grey'>- {data['time_display']}</font>"
        if data["count"] > 1:
            result += f" <font color='green'>({data['count']}æ¬¡)</font>"

        return result

    def _format_dingtalk(
        self,
        title: str,
        link_url: str,
        prefix: str,
        data: Dict,
        rank_display: str,
        show_source: bool
    ) -> str:
        """é’‰é’‰æ ¼å¼"""
        if link_url:
            formatted_title = f"[{title}]({link_url})"
        else:
            formatted_title = title

        if show_source:
            result = f"[{data['source_name']}] {prefix}{formatted_title}"
        else:
            result = f"{prefix}{formatted_title}"

        if rank_display:
            result += f" {rank_display}"
        if data["time_display"]:
            result += f" - {data['time_display']}"
        if data["count"] > 1:
            result += f" ({data['count']}æ¬¡)"

        return result

    def _format_wework(
        self,
        title: str,
        link_url: str,
        prefix: str,
        data: Dict,
        rank_display: str,
        show_source: bool
    ) -> str:
        """ä¼ä¸šå¾®ä¿¡æ ¼å¼"""
        return self._format_dingtalk(title, link_url, prefix, data, rank_display, show_source)

    def _format_telegram(
        self,
        title: str,
        link_url: str,
        prefix: str,
        data: Dict,
        rank_display: str,
        show_source: bool
    ) -> str:
        """Telegramæ ¼å¼"""
        if link_url:
            formatted_title = f'<a href="{link_url}">{html_escape(title)}</a>'
        else:
            formatted_title = title

        if show_source:
            result = f"[{data['source_name']}] {prefix}{formatted_title}"
        else:
            result = f"{prefix}{formatted_title}"

        if rank_display:
            result += f" {rank_display}"
        if data["time_display"]:
            result += f" <code>- {data['time_display']}</code>"
        if data["count"] > 1:
            result += f" <code>({data['count']}æ¬¡)</code>"

        return result

    def _format_ntfy(
        self,
        title: str,
        link_url: str,
        prefix: str,
        data: Dict,
        rank_display: str,
        show_source: bool
    ) -> str:
        """ntfyæ ¼å¼"""
        if link_url:
            formatted_title = f"[{title}]({link_url})"
        else:
            formatted_title = title

        if show_source:
            result = f"[{data['source_name']}] {prefix}{formatted_title}"
        else:
            result = f"{prefix}{formatted_title}"

        if rank_display:
            result += f" {rank_display}"
        if data["time_display"]:
            result += f" `- {data['time_display']}`"
        if data["count"] > 1:
            result += f" `({data['count']}æ¬¡)`"

        return result

    def _format_html(
        self,
        title: str,
        link_url: str,
        data: Dict,
        rank_display: str
    ) -> str:
        """HTMLæ ¼å¼"""
        escaped_title = html_escape(title)
        escaped_source_name = html_escape(data["source_name"])

        if link_url:
            escaped_url = html_escape(link_url)
            formatted_title = f'[{escaped_source_name}] <a href="{escaped_url}" target="_blank" class="news-link">{escaped_title}</a>'
        else:
            formatted_title = f'[{escaped_source_name}] <span class="no-link">{escaped_title}</span>'

        if rank_display:
            formatted_title += f" {rank_display}"
        if data["time_display"]:
            escaped_time = html_escape(data["time_display"])
            formatted_title += f" <font color='grey'>- {escaped_time}</font>"
        if data["count"] > 1:
            formatted_title += f" <font color='green'>({data['count']}æ¬¡)</font>"

        if data.get("is_new"):
            formatted_title = f"<div class='new-title'>ğŸ†• {formatted_title}</div>"

        return formatted_title

    def get_output_path(self, output_type: str, filename: str) -> Path:
        """è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Args:
            output_type: è¾“å‡ºç±»å‹ (html/txt)
            filename: æ–‡ä»¶å

        Returns:
            Path: æ–‡ä»¶è·¯å¾„
        """
        date_folder = format_date_folder()
        output_dir = Path("output") / date_folder / output_type
        output_dir.mkdir(parents=True, exist_ok=True)
        return output_dir / filename

    def generate_text_report(
        self,
        stats: List[WordGroupStatistic],
        total_titles: int,
        failed_ids: Optional[List[str]] = None,
        new_news_list: Optional[List[News]] = None,
        mode: str = "daily",
        is_daily_summary: bool = False
    ) -> Path:
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š

        Args:
            stats: è¯ç»„ç»Ÿè®¡åˆ—è¡¨
            total_titles: æ€»æ ‡é¢˜æ•°
            failed_ids: å¤±è´¥çš„å¹³å°IDåˆ—è¡¨
            new_news_list: æ–°å¢æ–°é—»åˆ—è¡¨
            mode: æ¨¡å¼
            is_daily_summary: æ˜¯å¦ä¸ºå½“æ—¥æ±‡æ€»

        Returns:
            Path: ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„
        """
        if is_daily_summary:
            if mode == "current":
                filename = "å½“å‰æ¦œå•æ±‡æ€».txt"
            elif mode == "incremental":
                filename = "å½“æ—¥å¢é‡.txt"
            else:
                filename = "å½“æ—¥æ±‡æ€».txt"
        else:
            filename = f"{format_time_filename()}.txt"

        file_path = self.get_output_path("txt", filename)

        report_data = self.prepare_report_data(stats, failed_ids, new_news_list, mode)

        content_lines = []

        # å†™å…¥è¯ç»„ç»Ÿè®¡
        for stat in report_data["stats"]:
            content_lines.append(f"{stat['word']} (å…±{stat['count']}æ¡)")
            content_lines.append("")

            for title_data in stat["titles"]:
                # ç®€å•çš„æ–‡æœ¬æ ¼å¼
                line = f"[{title_data['source_name']}] {title_data['title']}"
                if title_data["ranks"]:
                    min_rank = min(title_data["ranks"])
                    max_rank = max(title_data["ranks"])
                    if min_rank == max_rank:
                        line += f" [{min_rank}]"
                    else:
                        line += f" [{min_rank} - {max_rank}]"
                if title_data["time_display"]:
                    line += f" - {title_data['time_display']}"
                if title_data["count"] > 1:
                    line += f" ({title_data['count']}æ¬¡)"
                if title_data["url"]:
                    line += f" [URL:{title_data['url']}]"
                if title_data["mobile_url"]:
                    line += f" [MOBILE:{title_data['mobile_url']}]"

                content_lines.append(line)

            content_lines.append("")

        # å†™å…¥æ–°å¢æ–°é—»
        if report_data["new_titles"]:
            content_lines.append("==== æœ€æ–°æ‰¹æ¬¡æ–°å¢ ====")
            content_lines.append("")

            for source_data in report_data["new_titles"]:
                content_lines.append(f"{source_data['source_name']} (æ–°å¢{len(source_data['titles'])}æ¡)")
                content_lines.append("")

                for title_data in source_data["titles"]:
                    line = f"{title_data['title']}"
                    if title_data["ranks"]:
                        line += f" [{title_data['ranks'][0]}]"
                    if title_data["url"]:
                        line += f" [URL:{title_data['url']}]"
                    if title_data["mobile_url"]:
                        line += f" [MOBILE:{title_data['mobile_url']}]"

                    content_lines.append(line)

                content_lines.append("")

        # å†™å…¥å¤±è´¥ä¿¡æ¯
        if report_data["failed_ids"]:
            content_lines.append("==== ä»¥ä¸‹IDè¯·æ±‚å¤±è´¥ ====")
            content_lines.append(", ".join(report_data["failed_ids"]))

        # å†™å…¥æ–‡ä»¶
        with open(file_path, "w", encoding="utf-8") as f:
            f.write("\n".join(content_lines))

        return file_path

    def generate_json_report(
        self,
        stats: List[WordGroupStatistic],
        total_titles: int,
        failed_ids: Optional[List[str]] = None,
        new_news_list: Optional[List[News]] = None,
        mode: str = "daily",
        is_daily_summary: bool = False
    ) -> Tuple[Path, Path]:
        """ç”Ÿæˆ JSON æŠ¥å‘Š(æ±‡æ€»+å¢é‡)

        Args:
            stats: è¯ç»„ç»Ÿè®¡åˆ—è¡¨
            total_titles: æ€»æ ‡é¢˜æ•°
            failed_ids: å¤±è´¥çš„å¹³å°IDåˆ—è¡¨
            new_news_list: æ–°å¢æ–°é—»åˆ—è¡¨
            mode: æ¨¡å¼
            is_daily_summary: æ˜¯å¦ä¸ºå½“æ—¥æ±‡æ€»

        Returns:
            Tuple[Path, Path]: (æ±‡æ€»æ–‡ä»¶è·¯å¾„, å¢é‡æ–‡ä»¶è·¯å¾„)
        """
        # å‡†å¤‡æŠ¥å‘Šæ•°æ®
        report_data = self.prepare_report_data(stats, failed_ids, new_news_list, mode)

        # å½“å‰æ‰¹æ¬¡æ—¶é—´æˆ³
        now = datetime.now()
        batch_id = format_time_filename()  # å¦‚: "15æ—¶35åˆ†"

        # æ„å»ºå½“å‰æ‰¹æ¬¡æ•°æ®
        current_batch = self._build_batch_data(
            batch_id=batch_id,
            timestamp=now,
            report_data=report_data,
            total_titles=total_titles
        )

        # ä¿å­˜æ±‡æ€» JSON (è¿½åŠ æ¨¡å¼)
        summary_path = self._save_summary_json(current_batch, mode)

        # ä¿å­˜å¢é‡ JSON (è¦†å†™æ¨¡å¼)
        incremental_path = self._save_incremental_json(
            batch_id=batch_id,
            timestamp=now,
            report_data=report_data,
            mode=mode
        )

        return summary_path, incremental_path

    def _build_batch_data(
        self,
        batch_id: str,
        timestamp: datetime,
        report_data: Dict,
        total_titles: int
    ) -> Dict[str, Any]:
        """æ„å»ºå•ä¸ªæ‰¹æ¬¡çš„æ•°æ®ç»“æ„

        Args:
            batch_id: æ‰¹æ¬¡ID(å¦‚ "15æ—¶35åˆ†")
            timestamp: æ—¶é—´æˆ³
            report_data: prepare_report_data() è¿”å›çš„æ•°æ®
            total_titles: æ€»æ–°é—»æ•°

        Returns:
            Dict: æ‰¹æ¬¡æ•°æ®ç»“æ„
        """
        # è½¬æ¢è¯ç»„ç»Ÿè®¡æ•°æ®
        stats_list = []
        for stat in report_data["stats"]:
            news_list = []
            for title_data in stat["titles"]:
                news_item = {
                    "title": title_data["title"],
                    "url": title_data["url"],
                    "mobile_url": title_data["mobile_url"],
                    "platform": title_data["source_name"],  # å¹³å°æ˜¾ç¤ºåç§°
                    "rank": min(title_data["ranks"]) if title_data["ranks"] else 999,
                    "ranks": title_data["ranks"],  # æ‰€æœ‰æ’å
                    "occurrence_count": title_data["count"],  # å‡ºç°æ¬¡æ•°
                    "time_display": title_data["time_display"],
                    "is_new": title_data.get("is_new", False),
                }
                news_list.append(news_item)

            stats_list.append({
                "word_group": stat["word"],
                "count": stat["count"],
                "percentage": round(stat["percentage"], 2),
                "news_list": news_list,
            })

        return {
            "batch_id": batch_id,
            "timestamp": timestamp.isoformat(),
            "total_news_count": total_titles,
            "stats": stats_list,
        }

    def _save_summary_json(
        self,
        current_batch: Dict[str, Any],
        mode: str
    ) -> Path:
        """ä¿å­˜æ±‡æ€» JSON (è¿½åŠ æ¨¡å¼)

        æ±‡æ€»æ–‡ä»¶åŒ…å«å½“å¤©æ‰€æœ‰æ‰¹æ¬¡çš„å†å²æ•°æ®,æ¯æ¬¡æ‰§è¡Œæ—¶è¿½åŠ æ–°æ‰¹æ¬¡ã€‚

        Args:
            current_batch: å½“å‰æ‰¹æ¬¡æ•°æ®
            mode: è¿è¡Œæ¨¡å¼

        Returns:
            Path: æ±‡æ€»æ–‡ä»¶è·¯å¾„
        """
        # è·å–æ±‡æ€»æ–‡ä»¶è·¯å¾„
        summary_path = self.get_output_path("json", "news_summary.json")

        # è¯»å–ç°æœ‰æ±‡æ€»æ•°æ®
        if summary_path.exists():
            try:
                with open(summary_path, "r", encoding="utf-8") as f:
                    summary_data = json.load(f)
            except (json.JSONDecodeError, IOError):
                # æ–‡ä»¶æŸåæˆ–è¯»å–å¤±è´¥,é‡æ–°åˆå§‹åŒ–
                summary_data = self._init_summary_structure()
        else:
            # é¦–æ¬¡åˆ›å»º
            summary_data = self._init_summary_structure()

        # è¿½åŠ å½“å‰æ‰¹æ¬¡
        summary_data["batches"].append(current_batch)

        # æ›´æ–°å…ƒæ•°æ®
        summary_data["metadata"]["total_batches"] = len(summary_data["batches"])
        summary_data["metadata"]["last_update"] = current_batch["timestamp"]
        summary_data["metadata"]["total_news_count"] = sum(
            batch["total_news_count"] for batch in summary_data["batches"]
        )

        # åŸå­å†™å…¥(å…ˆå†™ä¸´æ—¶æ–‡ä»¶,å†é‡å‘½å)
        self._atomic_write_json(summary_path, summary_data)

        return summary_path

    def _save_incremental_json(
        self,
        batch_id: str,
        timestamp: datetime,
        report_data: Dict,
        mode: str
    ) -> Path:
        """ä¿å­˜å¢é‡ JSON (è¦†å†™æ¨¡å¼)

        å¢é‡æ–‡ä»¶ä»…åŒ…å«å½“å‰æ‰¹æ¬¡çš„æ–°å¢æ•°æ®,æ¯æ¬¡æ‰§è¡Œæ—¶å®Œå…¨è¦†å†™ã€‚

        Args:
            batch_id: æ‰¹æ¬¡ID
            timestamp: æ—¶é—´æˆ³
            report_data: prepare_report_data() è¿”å›çš„æ•°æ®
            mode: è¿è¡Œæ¨¡å¼

        Returns:
            Path: å¢é‡æ–‡ä»¶è·¯å¾„
        """
        incremental_path = self.get_output_path("json", "news_incremental.json")

        # æ„å»ºå¢é‡æ•°æ®(ä»…åŒ…å«æ ‡è®°ä¸ºæ–°å¢çš„æ–°é—»)
        incremental_stats = []
        for stat in report_data["stats"]:
            # åªä¿ç•™æ–°å¢æ–°é—»
            new_news_list = [
                title_data for title_data in stat["titles"]
                if title_data.get("is_new", False)
            ]

            if new_news_list:
                news_items = []
                for title_data in new_news_list:
                    news_items.append({
                        "title": title_data["title"],
                        "url": title_data["url"],
                        "mobile_url": title_data["mobile_url"],
                        "platform": title_data["source_name"],
                        "rank": min(title_data["ranks"]) if title_data["ranks"] else 999,
                        "ranks": title_data["ranks"],
                        "occurrence_count": title_data["count"],
                        "time_display": title_data["time_display"],
                    })

                incremental_stats.append({
                    "word_group": stat["word"],
                    "count": len(news_items),
                    "percentage": round(
                        len(news_items) / report_data.get("total_new_count", 1) * 100, 2
                    ) if report_data.get("total_new_count") else 0,
                    "news_list": news_items,
                })

        # æ„å»ºå®Œæ•´å¢é‡æ•°æ®ç»“æ„
        incremental_data = {
            "metadata": {
                "date": datetime.now().strftime("%Y-%m-%d"),
                "batch_id": batch_id,
                "timestamp": timestamp.isoformat(),
                "new_news_count": report_data.get("total_new_count", 0),
            },
            "stats": incremental_stats,
        }

        # åŸå­å†™å…¥
        self._atomic_write_json(incremental_path, incremental_data)

        return incremental_path

    def _init_summary_structure(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–æ±‡æ€»æ•°æ®ç»“æ„

        Returns:
            Dict: åˆå§‹åŒ–çš„æ±‡æ€»æ•°æ®
        """
        return {
            "metadata": {
                "date": datetime.now().strftime("%Y-%m-%d"),
                "total_batches": 0,
                "total_news_count": 0,
                "last_update": None,
            },
            "batches": [],
        }

    def _atomic_write_json(self, file_path: Path, data: Dict[str, Any]) -> None:
        """åŸå­å†™å…¥ JSON æ–‡ä»¶

        å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶,æˆåŠŸåå†é‡å‘½å,é˜²æ­¢å†™å…¥å¤±è´¥å¯¼è‡´æ•°æ®ä¸¢å¤±ã€‚

        Args:
            file_path: ç›®æ ‡æ–‡ä»¶è·¯å¾„
            data: è¦å†™å…¥çš„æ•°æ®
        """
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        file_path.parent.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶(åœ¨åŒä¸€ç›®å½•ä¸‹,ç¡®ä¿åŸå­ rename)
        fd, temp_path = tempfile.mkstemp(
            dir=file_path.parent,
            prefix=".tmp_",
            suffix=".json"
        )

        try:
            # å†™å…¥ä¸´æ—¶æ–‡ä»¶
            with os.fdopen(fd, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            # åŸå­é‡å‘½å
            os.replace(temp_path, file_path)
        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(temp_path)
            except:
                pass
            raise e
